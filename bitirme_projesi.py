# -*- coding: utf-8 -*-
"""bitirme_projesi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vspuf4vYDnGngQIw_oxqazAcuE0QX4Yd
"""

url="https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"

import tensorflow as tf

dataset=tf.keras.utils.get_file("aclImdb_v1",
                         url,
                         untar=True,
                         cache_dir=".",
                         cache_subdir="")

import os
dataset_dir=os.path.join(os.path.dirname(dataset),"aclImdb")

os.listdir(dataset_dir)

train_dir=os.path.join(dataset_dir,"train")
os.listdir(train_dir)

## etiketi pozitif olan yorumu getiriyor
sample_file=os.path.join(train_dir,"pos/1181_9.txt")
with open (sample_file) as f:
  print(f.read())

remove_dir=os.path.join(train_dir,"unsup")
import shutil
shutil.rmtree(remove_dir)

##Loading the dataset

##kaçarlı gruplara ayrıldığını batchsize,rastgele seçilen değerlerin aynı kalmasını seed sağlıyor,
batch_size=32
seed=42
raw_train_ds=tf.keras.preprocessing.text_dataset_from_directory(
    train_dir,
    batch_size=batch_size,
    validation_split=0.2,
    subset="training",
    seed=seed

)

##veri setindeki 3 yorumu getiriyor ve altlarına etiketini gösteriyor olumlu mu olumsuz mu diye.
import numpy as np
for text_batch, label_batch in raw_train_ds.take(1):
  for i in range (3):
    print("Review",text_batch.numpy()[i])
    print("Label",label_batch.numpy()[i])

##validasyon ile modelin hiperparametrelerini ayarlıyoruz
raw_val_ds=tf.keras.preprocessing.text_dataset_from_directory(
    train_dir,
    batch_size=batch_size,
    validation_split=0.2,
    subset="validation",
    seed=seed

)

test_dir=os.path.join(dataset_dir,"test")
raw_test_ds=tf.keras.preprocessing.text_dataset_from_directory(
    test_dir,
    batch_size=batch_size,

)

##data preprocessing

##veri temizleme
import re
import string
def custom_standardization(input_data):
  lowercase=tf.strings.lower(input_data)
  stripped_html=tf.strings.regex_replace(lowercase,'<br/>','')
  return tf.strings.regex_replace(stripped_html,'[%s]' % re.escape(string.punctuation), '')

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
max_features=10000
sequence_length=250
vectorize_layer= TextVectorization(
    standardize=custom_standardization,
    max_tokens= max_features,
    output_mode="int",
    output_sequence_length=sequence_length

)

##örnek veri setini temizlemiş olduk.
 train_text = raw_train_ds.map(lambda x, y: x)
 vectorize_layer.adapt(train_text)

def vectorize_text(text,label):
  text=tf.expand_dims(text,-1)
  return vectorize_layer(text),label

text_batch, label_batch= next(iter(raw_train_ds))

first_review, first_label= text_batch[0], label_batch[0]
print("Review", first_review)
print("Label", raw_train_ds.class_names[first_label])
print("Vectorized review", vectorize_text(first_review ,first_label))

print("128 -->",vectorize_layer.get_vocabulary()[128])
print("1312 -->",vectorize_layer.get_vocabulary()[1312])
print("Vocabulary size:{}".format(len(vectorize_layer.get_vocabulary())))

train_ds =raw_train_ds.map(vectorize_text)
val_ds =raw_val_ds.map(vectorize_text)
test_ds =raw_test_ds.map(vectorize_text)

##configure the dataset for performance

##cache metodu verileri diskten yükledikten sonra bellekte tutar.
##prefetch metodu ise eğitim esnasında veri ön işlemeyi ve model uygulamayı birlikte yapar.
AUTOTUNE= tf.data.AUTOTUNE
train_ds=train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds= val_ds.cache().prefetch(buffer_size=AUTOTUNE)
test_ds= test_ds.cache().prefetch(buffer_size=AUTOTUNE)

##CREATİNG THE MODEL

embedding_dim=16
model=tf.keras.Sequential([

  tf.keras.layers.Embedding(max_features +1, embedding_dim),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.GlobalAveragePooling1D(),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(1)
                          ])

##10.000 girişimiz var 16 çıkışımız var o yüzden 10.000*16+16=160.016 olur. 16 çıkış +1bayes =17 olur.
model.summary()

## COMPİLİNG THE MODEL

model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer="adam",
              metrics=tf.metrics.BinaryAccuracy(threshold=0.0)
              )

##training the model

epochs=10
history=model.fit(
   train_ds,
   validation_data=val_ds,
   epochs=epochs
)

loss , accuracy=model.evaluate(test_ds)
print("Loss",loss)
print("Accuracy: ",accuracy)

## PLOT OF ACCURACY AND LOSS

history_dict=history.history
history_dict.keys()

import matplotlib.pyplot as plt
acc= history_dict["binary_accuracy"]
val_acc=history_dict["val_binary_accuracy"]
loss=history_dict["loss"]
val_loss=history_dict["val_loss"]

epochs= range(1,len(acc)+1)
plt.plot(epochs , loss, "bo", label="Training loss")
plt.plot(epochs , val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show(

)

plt.plot(epochs , acc, "bo", label="Training acc")
plt.plot(epochs , val_acc, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend("Lower right")
plt.show(

)
##eğitim verilerinin doğruluğu validasyonun doğruluğundan fazla olduğu için bu modelin ezber yaptığı anlamına gelir.

## exporting the model

export_model=tf.keras.Sequential([
                                  vectorize_layer,
                                  model,
                                  tf.keras.layers.Activation("sigmoid")
])

export_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False)
                     ,optimizer="adam",
                     metrics=["accuracy"])

loss,accuracy=export_model.evaluate(raw_test_ds)
print(accuracy)

## predicting new data

examples = [
            "The movie was perfect",
            "The movie was okay",
            "The movie was bad",
            "I really liked this movie."
]

export_model.predict(examples)

